{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0e0971",
   "metadata": {},
   "source": [
    "## Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff616e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shapely\n",
    "import shapely.speedups\n",
    "from sklearn import linear_model\n",
    "import geopandas as gpd\n",
    "import math\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5d99ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data -- Site 1\n",
    "dfout_s1 = pd.read_csv('Data/PVData/Site1/PVOut_46834.csv', header=None)\n",
    "dfout_s1 = dfout_s1.drop(axis=1, columns=[3,4,6,7,8,9,10,11,12,13])\n",
    "dfout_s1 = dfout_s1.drop(axis=0, index=[item for item in range(32,49+1)])\n",
    "dfout_s1 = dfout_s1.drop(axis=0, index=[0])\n",
    "dfout_s1 = dfout_s1.loc[::-1].reset_index(drop=True)\n",
    "\n",
    "dfsys_s1 = pd.read_csv('Data/PVData/Site1/PVSystem_46834.csv', header=None)\n",
    "\n",
    "weather_s1 = pd.read_csv('Data/PVData/Site1/NSRDBout_s1.csv', header=None)\n",
    "weather_s1 = weather_s1.drop(axis=0, index=[0,1,2])\n",
    "weather_s1 = weather_s1.drop(axis=1, columns=[item for item in range(14,47)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data -- Site 2\n",
    "dfout_s2 = pd.read_csv('Data/PVData/Site2/PVOut_3445.csv', header=None)\n",
    "dfout_s2 = dfout_s2.drop(axis=1, columns=[3,4,6,7,8,9,10,11,12,13])\n",
    "dfout_s2 = dfout_s2.loc[::-1].reset_index(drop=True)\n",
    "\n",
    "dfsys_s2 = pd.read_csv('Data/PVData/Site2/PVSystem_3445.csv', header=None)\n",
    "\n",
    "weather_s2 = pd.read_csv('Data/PVData/Site2/NSRDBout_s2.csv', header=None)\n",
    "weather_s2 = weather_s2.drop(axis=0, index=[0,1,2])\n",
    "weather_s2 = weather_s2.drop(axis=1, columns=[item for item in range(14,47)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea5ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputPanel(systemCapacity,energyGen):\n",
    "    # We are calculating the output per panel assuming that each panel has a capactiy of 300 Watts\n",
    "    wattage = 300\n",
    "    newPanel = systemCapacity/wattage\n",
    "    outPerPanel = energyGen/newPanel\n",
    "    \n",
    "    return int(round(int(outPerPanel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434d59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns daily averages for meteorlogical data over the course of a specified month\n",
    "## also added the energy output to this dataset\n",
    "# One must specify the MONTH and the NUMBER OF DAYS IN THE MONTH\n",
    "def daily(weatherData,pvOutData,pvSysData,year,month,numOfDaysMonth):\n",
    "    \n",
    "    monthstr = str(month)\n",
    "    \n",
    "    # Start of with monthly dataset\n",
    "    mask = weatherData[2].values == monthstr\n",
    "    pos = np.flatnonzero(mask)\n",
    "    monthData = weatherData.iloc[pos]\n",
    "\n",
    "    dailyData = pd.DataFrame(columns = ['Year','Month','Day','lat','lon','GHI','DHI','DNI',\n",
    "                                        'Energy Gen (watt hr)',\n",
    "                                        'Wind Speed','Temperature','Solar Zenith Angle',\n",
    "                                        'Pressure','Relative Humidity'])    \n",
    "        \n",
    "    # Now the Daily Datasets\n",
    "    for i in range(1, numOfDaysMonth+1):\n",
    "        # Weather Data\n",
    "        dayMask = monthData[3].values == str(i)\n",
    "        pos = np.flatnonzero(dayMask)\n",
    "        dayData = monthData.iloc[pos]\n",
    "            \n",
    "        #PV Data\n",
    "        gen = outputPanel(pvSysData.iloc[0,1], pvOutData.iloc[(i-1),1])\n",
    "        \n",
    "        #PV and Weather Data combined\n",
    "        df2 = pd.DataFrame([{'Year':year, 'Month':month, 'Day':i, \n",
    "                             'lat':pvSysData.iloc[0,13],'lon':pvSysData.iloc[0,14],\n",
    "                             'Energy Gen (watt hr)':gen, \n",
    "                             'GHI':average(dayData,6), 'DHI':average(dayData,7), \n",
    "                             'DNI':average(dayData,8), 'Wind Speed':average(dayData,9), \n",
    "                             'Temperature':average(dayData,10), 'Solar Zenith Angle':average(dayData,11),\n",
    "                             'Pressure':average(dayData,12), 'Relative Humidity':average(dayData,13)}])\n",
    "        \n",
    "        dailyData = dailyData.append(df2, ignore_index=True)\n",
    "        \n",
    "    return dailyData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cc39f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(data, col):\n",
    "    col = str(col)\n",
    "    co = data[col].tolist()\n",
    "    intList = [float(item) for item in co]\n",
    "    avg = sum(intList)/len(intList)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26f5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1  = daily(weather_s1,dfout_s1,dfsys_s1,2020,5,31)\n",
    "dataset_s2 = daily(weather_s2,dfout_s2,dfsys_s2,2020,6,30)\n",
    "dataset = dataset_s1.append(dataset_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865d492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'Day',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'GHI',\n",
       " 'DHI',\n",
       " 'DNI',\n",
       " 'Energy Gen (watt hr)',\n",
       " 'Wind Speed',\n",
       " 'Temperature',\n",
       " 'Solar Zenith Angle',\n",
       " 'Pressure',\n",
       " 'Relative Humidity']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = dataset.columns.to_list()\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146da7a",
   "metadata": {},
   "source": [
    "## Multiple Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91eb85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in variables\n",
    "x = dataset[['GHI','Relative Humidity','Pressure','Temperature','Solar Zenith Angle','Wind Speed','DNI','DHI']]\n",
    "y = dataset['Energy Gen (watt hr)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc64630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r squared:  0.930235077024612 \n",
      "Variables:  ['GHI', 'Relative Humidity', 'Pressure', 'Temperature', 'Solar Zenith Angle', 'Wind Speed', 'DNI', 'DHI']\n"
     ]
    }
   ],
   "source": [
    "## Load in regression model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x,y)\n",
    "\n",
    "# r^2 value\n",
    "r2 = regr.score(x,y)\n",
    "print('r squared: ',r2, '\\nVariables:ho', x.columns.to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37caf4",
   "metadata": {},
   "source": [
    "## Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup -- getting rid of states that are not in the contiguous US\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "shapely.speedups.enable()\n",
    "# Get shape file of Continguous US\n",
    "df = gpd.read_file('CartographicBoundries/US_State/cb_2018_us_state_500k.shp')\n",
    "df = df.drop([37,38,44,45,13,27,42])\n",
    "contUSdf = df.dissolve()\n",
    "\n",
    "## upper and lower bound of coords in the contiguous US\n",
    "bound = pd.DataFrame(contUSdf.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d627bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of the boundries from the Polygon\n",
    "def find_boundary(index):\n",
    "    minx = bound.iat[index, 0]\n",
    "    miny = bound.iat[index, 1]\n",
    "    maxx = bound.iat[index, 2]\n",
    "    maxy = bound.iat[index, 3]\n",
    "    return minx, miny, maxx, maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28de4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should be named 'factor' ... but whatever\n",
    "def multiples(n):\n",
    "    l = [*range(1, n+1)]\n",
    "    multiples = []\n",
    "    for i in range(1, l[len(l)-1]+1):\n",
    "        if n % i == 0:\n",
    "            multiples.append(i)\n",
    "    return multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5777ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of subdivisions to make,\n",
    "# Uses the middle 2 elements of the mutiples list ...\n",
    "# ... (created by above function)\n",
    "def num_of_divisions(area):\n",
    "    m = multiples(area)\n",
    "    w_n = m[len(m) // 2]\n",
    "    h_n = m[len(m) // 2]\n",
    "    if (len(m) % 2) == 1:\n",
    "        return w_n, h_n\n",
    "    else:\n",
    "        h_n = m[(len(m) // 2) - 1]\n",
    "    return w_n, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f36d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of divisions .. coordinate difference (not standard for both latitude and longitude)\n",
    "def len_of_div(bounds, width_n, height_n):\n",
    "    a = bounds[0]\n",
    "    # width subdivision\n",
    "    width = bounds[2] - bounds[0]\n",
    "    length_of_each_div_w = width/width_n\n",
    "    # length subdivision\n",
    "    height = bounds[3] - bounds[1]\n",
    "    length_of_each_div_h = height/height_n\n",
    "    return length_of_each_div_w, length_of_each_div_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ade99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns list of coords to use as datapoints, each respective position in the lat and lon corresponds to the same datapoint\n",
    "## Future - - perhaps make a sigular list with tuples representing each datapoint rather than haveing two datasets (one for lat and other for lon)\n",
    "def coord_of_div(len_div, bounds, num_of_divs):\n",
    "    ## formula for first point = a + (1/2)length\n",
    "    initial_coord_w = bounds[0] + (0.5 * len_div[0])\n",
    "    initial_coord_h = bounds[1] + (0.5 * len_div[1])\n",
    "    w = []\n",
    "    h = []\n",
    "    for i in range (num_of_divs[0]):\n",
    "        w.append(initial_coord_w + (i * len_div[0]))\n",
    "    for i in range (num_of_divs[1]):\n",
    "        h.append(initial_coord_h + (i * len_div[1]))\n",
    "    return w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a8668fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters out datapoint that are not in the contiguous USA\n",
    "def point_dataframe(longitude, latitude):\n",
    "    point = pd.DataFrame(columns = ['lon', 'lat'])\n",
    "    for x in longitude:\n",
    "        for y in latitude:\n",
    "            point2 = pd.DataFrame([{'lon':x, 'lat':y}])\n",
    "            point = point.append(point2, ignore_index = True)\n",
    "    gdf = gpd.GeoDataFrame(point,\n",
    "            geometry = gpd.points_from_xy(point.lon, point.lat))\n",
    "    ## Check if point is in main polygon\n",
    "    # pip = 'point in polygon'\n",
    "    pip_mask = gdf.within(contUSdf.loc[0, 'geometry'])\n",
    "    pip_data = gdf.loc[pip_mask]\n",
    "    return pip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b34f25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the NSRDB API to get weather data for wach of the points, yearly averages\n",
    "# future notes -- plan on getting monthly averages\n",
    "def get_data(list_lat, list_lon):\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['lat','lon','GHI','DHI','DNI',\n",
    "                                 'Wind Speed','Temperature',\n",
    "                                 'Solar Zenith Angle','Pressure',\n",
    "                                 'Relative Humidity'])\n",
    "    \n",
    "    if len(list_lat) != len(list_lon):\n",
    "        print(\"Error --- shape of list_lat does not match that of list_lons\")\n",
    "    else:\n",
    "        for i in range(len(list_lat)):\n",
    "                # Declare all variables as strings. Spaces must be replaced with '+', i.e., change 'John Smith' to 'John+Smith'.\n",
    "                # Define the lat, long of the location and the year\n",
    "                latitude = list_lat[i]\n",
    "                longitude = list_lon[i]\n",
    "                lat,lon,year = latitude, longitude, 2010\n",
    "                # You must request an NSRDB api key from the link above\n",
    "                api_key = 'Ys1FBygszkOmc2ifUvWD8LdkRFWGaIbNByDa5Ddc'\n",
    "                # Set the attributes to extract (e.g., dhi, ghi, etc.), separated by commas.\n",
    "                attributes = 'ghi,dhi,dni,wind_speed,air_temperature,solar_zenith_angle,surface_pressure,relative_humidity'            # Choose year of data\n",
    "                year = '2020'\n",
    "                # Set leap year to true or false. True will return leap day data if present, false will not.\n",
    "                leap_year = 'false'\n",
    "                # Set time interval in minutes, i.e., '30' is half hour intervals. Valid intervals are 30 & 60.\n",
    "                interval = '60'\n",
    "                # Specify Coordinated Universal Time (UTC), 'true' will use UTC, 'false' will use the local time zone of the data.\n",
    "                # NOTE: In order to use the NSRDB data in SAM, you must specify UTC as 'false'. SAM requires the data to be in the\n",
    "                # local time zone.\n",
    "                utc = 'true'\n",
    "                # Your full name, use '+' instead of spaces.\n",
    "                your_name = 'Shrey+Poshiya'\n",
    "                # Your reason for using the NSRDB.\n",
    "                reason_for_use = 'personal+project'\n",
    "                # Your affiliation\n",
    "                your_affiliation = 'Santa+Fe+Preparatory+School'\n",
    "                # Your email address\n",
    "                your_email = 'shreyposh@gmail.com'\n",
    "                # Please join our mailing list so we can keep you up-to-date on new developments.\n",
    "                mailing_list = 'false'\n",
    "                # Declare url string\n",
    "                url = 'https://developer.nrel.gov/api/solar/nsrdb_psm3_download.csv?wkt=POINT({lon}%20{lat})&names={year}&leap_day={leap}&interval={interval}&utc={utc}&full_name={name}&email={email}&affiliation={affiliation}&mailing_list={mailing_list}&reason={reason}&api_key={api}&attributes={attr}'.format(year=year, lat=lat, lon=lon, leap=leap_year, interval=interval, utc=utc, name=your_name, email=your_email, mailing_list=mailing_list, affiliation=your_affiliation, reason=reason_for_use, api=api_key, attr=attributes)\n",
    "                # Return just the first 2 lines to get metadata:\n",
    "                info = pd.read_csv(url, skiprows=2)\n",
    "                # See metadata for specified properties, e.g., timezone and elevation\n",
    "                info = info.drop(axis=0, index=[0])\n",
    "                print(info)\n",
    "\n",
    "                ## Make a new dataframe, with the same columns as the first dataframe ... then concat\n",
    "                df2 = pd.DataFrame([{'lat':latitude, 'lon':longitude, 'GHI':average(info, 'GHI'), \n",
    "                                     'DHI':average(info, 'DHI'), \n",
    "                                     'DNI':average(info, 'DNI'),\n",
    "                                     'Wind Speed':average(info, 'Wind Speed'),\n",
    "                                     'Temperature':average(info, 'Temperature'),\n",
    "                                     'Solar Zenith Angle':average(info, 'Solar Zenith Angle'),\n",
    "                                     'Pressure':average(info, 'Pressure'),\n",
    "                                     'Relative Humidity':average(info, 'Relative Humidity')}])\n",
    "                df = df.append(df2, ignore_index = True)\n",
    "                \n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
